# DL_Compiler_and_Hardware Study

This is a repository of the study "DL Compiler and Hardware". The goal of this study is to understand the acceleration of nerual networks with DL Compiler. The topic of acceleration includes `On-Device AI`,`DL Compiler`, `TVM`, `ONNX` , `Compiler`, `PIM/CIM`, `NPU`. Our study is based on recent papaers (Under recent two years). We discuss topics such as `HW architecture`, `SW acceleration`.


## Presentation Order
When | Who | What | Links | Issue # | Etc.
---- | --------- | ----------------------------------------- | ----------------------- | --------------------- | ----
7/5 | 박상수 | Introduction to Efficient AI Study | - | #1 | -
7/19 | - | Challenges on Optimization of LLM Inference | - | #2 | -
8/02 | - | How to Improve Sampling Speed of Diffusion Models | - | #3 | -
8/16 | - | - | - | - | -
8/30 | - | DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation | https://arxiv.org/pdf/2209.10797.pdf | #5 | -
9/13 | - | LoRA: Low-Rank Adaptation of Large Language Models | https://openreview.net/forum?id=nZeVKeeFYf9 | #6 | -
9/27 | - | ClimbQ: Class Imbalanced Quantization Enabling Robustness on Efficient Inferences | https://openreview.net/forum?id=F7NQzsl334D | #7 | -
10/11 | - | NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers (CVPR-2023) | https://arxiv.org/abs/2211.16056 | #8 | -
10/25 |  - | FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization | N/A yet | - | -
7/5 | - | Reparameterized Technique | https://arxiv.org/abs/2101.03697 , https://arxiv.org/pdf/2212.01593.pdf | #10 | -
7/12 | - | - | - | - | -
7/19 | - | - | https://arxiv.org/abs/2302.01318, https://arxiv.org/abs/2211.17192 | #12 | -
7/26 | - | - | - | - | -
